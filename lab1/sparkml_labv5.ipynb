{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With Spark ML\n",
    "In this lab assignment, you will complete a project by going through the following steps:\n",
    "1. Get the data.\n",
    "2. Discover the data to gain insights.\n",
    "3. Prepare the data for Machine Learning algorithms.\n",
    "4. Select a model and train it.\n",
    "5. Fine-tune your model.\n",
    "6. Present your solution.\n",
    "\n",
    "As a dataset, we use the California Housing Prices dataset from the StatLib repository. This dataset was based on data from the 1990 California census. The dataset has the following columns\n",
    "1. `longitude`: a measure of how far west a house is (a higher value is farther west)\n",
    "2. `latitude`: a measure of how far north a house is (a higher value is farther north)\n",
    "3. `housing_,median_age`: median age of a house within a block (a lower number is a newer building)\n",
    "4. `total_rooms`: total number of rooms within a block\n",
    "5. `total_bedrooms`: total number of bedrooms within a block\n",
    "6. `population`: total number of people residing within a block\n",
    "7. `households`: total number of households, a group of people residing within a home unit, for a block\n",
    "8. `median_income`: median income for households within a block of houses\n",
    "9. `median_house_value`: median house value for households within a block\n",
    "10. `ocean_proximity`: location of the house w.r.t ocean/sea\n",
    "\n",
    "---\n",
    "# 1. Get the data\n",
    "Let's start the lab by loading the dataset. The can find the dataset at `data/housing.csv`. To infer column types automatically, when you are reading the file, you need to set `inferSchema` to true. Moreover enable the `header` option to read the columns' name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing = [longitude: double, latitude: double ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 8 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n",
      "Predicting\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val housing = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\")load(\"data/housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Discover the data to gain insights\n",
    "Now it is time to take a look at the data. In this step we are going to take a look at the data a few different ways:\n",
    "* See the schema and dimension of the dataset\n",
    "* Look at the data itself\n",
    "* Statistical summary of the attributes\n",
    "* Breakdown of the data by the categorical attribute variable\n",
    "* Find the correlation among different attributes\n",
    "* Make new attributes by combining existing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Schema and dimension\n",
    "Print the schema of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|          269700.0|       NEAR BAY|\n",
      "|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|          299200.0|       NEAR BAY|\n",
      "|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|\n",
      "|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|          226700.0|       NEAR BAY|\n",
      "|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|          261100.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|          281500.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|          241800.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|          213500.0|       NEAR BAY|\n",
      "|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|          159200.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|\n",
      "|  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|          152500.0|       NEAR BAY|\n",
      "|  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|          155500.0|       NEAR BAY|\n",
      "|  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|          158700.0|       NEAR BAY|\n",
      "|  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|          162900.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Look at the data\n",
    "Print the first five records of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records with population more than 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.where(\"population > 10000\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Statistical summary\n",
    "Print a summary of the table statistics for the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. You can use the `describe` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|housing_median_age|       total_rooms|median_house_value|        population|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|             20640|             20640|             20640|             20640|\n",
      "|   mean|28.639486434108527|2635.7630813953488|206855.81690891474|1425.4767441860465|\n",
      "| stddev| 12.58555761211163|2181.6152515827944|115395.61587441359|  1132.46212176534|\n",
      "|    min|               1.0|               2.0|           14999.0|               3.0|\n",
      "|    max|              52.0|           39320.0|          500001.0|           35682.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a = [summary: string, housing_median_age: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, housing_median_age: string ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val a=housing.select(\"housing_median_age\",\"total_rooms\",\"median_house_value\",\"population\").describe()\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the maximum age (`housing_median_age`), the minimum number of rooms (`total_rooms`), and the average of house values (`median_house_value`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------+-----------------------+\n",
      "|max(housing_median_age)|min(total_rooms)|avg(median_house_value)|\n",
      "+-----------------------+----------------+-----------------------+\n",
      "|                   52.0|             2.0|     206855.81690891474|\n",
      "+-----------------------+----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "housing.select(max(\"housing_median_age\"),min(\"total_rooms\"),avg(\"median_house_value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Breakdown the data by categorical data\n",
    "Print the number of houses in different areas (`ocean_proximity`), and sort them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|ocean_proximity|count(ocean_proximity)|\n",
      "+---------------+----------------------+\n",
      "|     NEAR OCEAN|                  2658|\n",
      "|       NEAR BAY|                  2290|\n",
      "|         ISLAND|                     5|\n",
      "|         INLAND|                  6551|\n",
      "|      <1H OCEAN|                  9136|\n",
      "+---------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.groupBy(\"ocean_proximity\").agg(count(\"ocean_proximity\")).sort(desc(\"ocean_proximity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average value of the houses (`median_house_value`) in different areas (`ocean_proximity`), and call the new column `avg_value` when print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|          avg_vale|\n",
      "+---------------+------------------+\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|         ISLAND|          380440.0|\n",
      "|         INLAND|124805.39200122119|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.groupBy(\"ocean_proximity\").agg(avg(\"median_house_value\") as \"avg_vale\").sort(desc(\"ocean_proximity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the above question in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|         ISLAND|          380440.0|\n",
      "|         INLAND|124805.39200122119|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "housing.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"select ocean_proximity,avg(median_house_value) as avg_value from df group by ocean_proximity order by ocean_proximity desc \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Correlation among attributes\n",
    "Print the correlation among the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. To do so, first you need to put these attributes into one vector. Then, compute the standard correlation coefficient (Pearson) between every pair of attributes in this new vector. To make a vector of these attributes, you can use the `VectorAssembler` Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|            features|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|[41.0,880.0,45260...|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|[21.0,7099.0,3585...|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|[52.0,1467.0,3521...|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|[52.0,1274.0,3413...|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|[52.0,1627.0,3422...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va = vecAssembler_3d05504a1288\n",
       "housingAttrs = [longitude: double, latitude: double ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 9 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"housing_median_age\",\"total_rooms\",\"median_house_value\")).setOutputCol(\"features\")\n",
    "\n",
    "val housingAttrs = va.transform(housing)\n",
    "\n",
    "housingAttrs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard correlation coefficient:\n",
      " 1.0                   -0.36126220122231784  0.10562341249318154  \n",
      "-0.36126220122231784  1.0                   0.13415311380654338  \n",
      "0.10562341249318154   0.13415311380654338   1.0                  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "coeff = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0                   -0.36126220122231784  0.10562341249318154\n",
       "-0.36126220122231784  1.0                   0.13415311380654338\n",
       "0.10562341249318154   0.13415311380654338   1.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.linalg.Matrix\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val Row(coeff: Matrix) = Correlation.corr(housingAttrs,\"features\").head()\n",
    "\n",
    "println(s\"The standard correlation coefficient:\\n ${coeff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Combine and make new attributes\n",
    "Now, let's try out various attribute combinations. In the given dataset, the total number of rooms in a block is not very useful, if we don't know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful, and we want to compare it to the number of rooms. And the population per household seems like also an interesting attribute combination to look at. To do so, add the three new columns to the dataset as below. We will call the new dataset the `housingExtra`.\n",
    "```\n",
    "rooms_per_household = total_rooms / households\n",
    "bedrooms_per_room = total_bedrooms / total_rooms\n",
    "population_per_household = population / households\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rooms_per_household = total_rooms / households;\n",
    "bedrooms_per_room = total_bedrooms / total_rooms;\n",
    "population_per_household = population / households;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------------+\n",
      "|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
      "+-------------------+-------------------+------------------------+\n",
      "|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
      "|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
      "|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
      "| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
      "|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
      "+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "housingCol1 = [longitude: double, latitude: double ... 9 more fields]\n",
       "housingCol2 = [longitude: double, latitude: double ... 10 more fields]\n",
       "housingExtra = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 11 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val housingCol1 = housing.withColumn(\"rooms_per_household\",expr(\"total_rooms/households\"))\n",
    "val housingCol2 = housingCol1.withColumn(\"bedrooms_per_room\",expr(\"total_bedrooms/total_rooms\"))\n",
    "val housingExtra = housingCol2.withColumn(\"population_per_household\",expr(\"population/households\"))\n",
    "\n",
    "housingExtra.select(\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prepare the data for Machine Learning algorithms\n",
    "Before going through the Machine Learning steps, let's first rename the label column from `median_house_value` to `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "renamedHousing = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 11 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val renamedHousing = housingExtra.withColumnRenamed(\"median_house_value\",\"label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to separate the numerical attributes from the categorical attribute (`ocean_proximity`) and keep their column names in two different lists. Moreover, sice we don't want to apply the same transformations to the predictors (features) and the label, we should remove the label attribute from the list of predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colLabel = label\n",
       "colCat = ocean_proximity\n",
       "colNum = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// label columns\n",
    "val colLabel = \"label\"\n",
    "\n",
    "// categorical columns\n",
    "val colCat = \"ocean_proximity\"\n",
    "\n",
    "// numerical columns\n",
    "val colNum = renamedHousing.columns.filter(_ != colLabel).filter(_ != colCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Prepare continuse attributes\n",
    "### Data cleaning\n",
    "Most Machine Learning algorithms cannot work with missing features, so we should take care of them. As a first step, let's find the columns with missing values in the numerical attributes. To do so, we can print the number of missing values of each continues attributes, listed in `colNum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude\n",
      "0\n",
      "latitude\n",
      "0\n",
      "housing_median_age\n",
      "0\n",
      "total_rooms\n",
      "0\n",
      "total_bedrooms\n",
      "207\n",
      "population\n",
      "0\n",
      "households\n",
      "0\n",
      "median_income\n",
      "0\n",
      "rooms_per_household\n",
      "0\n",
      "bedrooms_per_room\n",
      "207\n",
      "population_per_household\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "for (c <- colNum) {\n",
    "    println(c)\n",
    "   val a =renamedHousing.where(renamedHousing.col(c).isNull).count()\n",
    "\n",
    "   println(a) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observerd above, the `total_bedrooms` and `bedrooms_per_room` attributes have some missing values. One way to take care of missing values is to use the `Imputer` Transformer, which completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are located. To use it, you need to create an `Imputer` instance, specifying that you want to replace each attribute's missing values with the \"median\" of that attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|total_bedrooms|  bedrooms_per_room|\n",
      "+--------------+-------------------+\n",
      "|         129.0|0.14659090909090908|\n",
      "|        1106.0|0.15579659106916466|\n",
      "|         190.0|0.12951601908657123|\n",
      "|         235.0|0.18445839874411302|\n",
      "|         280.0| 0.1720958819913952|\n",
      "+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "imputer = imputer_8383f37ffc97\n",
       "imputedHousing = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 11 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val imputer = new Imputer().setStrategy(\"median\").setInputCols(Array(\"total_bedrooms\",\"bedrooms_per_room\")).setOutputCols(Array(\"total_bedrooms\",\"bedrooms_per_room\"))                                    \n",
    "val imputedHousing = imputer.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "imputedHousing.select(\"total_bedrooms\", \"bedrooms_per_room\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don't perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the label attribues is generally not required.\n",
    "\n",
    "One way to get all attributes to have the same scale is to use standardization. In standardization, for each value, first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. To do this, we can use the `StandardScaler` Estimator. To use `StandardScaler`, again we need to convert all the numerical attributes into a big vectore of features using `VectorAssembler`, and then call `StandardScaler` on that vactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, label, ocean_proximity, rooms_per_household, bedrooms_per_room, population_per_household)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputedHousing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|        featuresFlat|              scalar|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-122.22,37.86,21...|[-61.002278409814...|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-122.24,37.85,52...|[-61.012260782324...|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-122.25,37.85,52...|[-61.017251968579...|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-122.25,37.85,52...|[-61.017251968579...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va = vecAssembler_fc5867a69adf\n",
       "featuredHousing = [longitude: double, latitude: double ... 12 more fields]\n",
       "scaler = stdScal_082c13772a7c\n",
       "scaledHousing = [longitude: double, latitude: double ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 13 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(colNum).setOutputCol(\"featuresFlat\")\n",
    "val featuredHousing = va.transform(imputedHousing)\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"featuresFlat\").setOutputCol(\"scalar\")\n",
    "val scaledHousing = scaler.fit(featuredHousing).transform(featuredHousing)\n",
    "\n",
    "scaledHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Prepare categorical attributes\n",
    "After imputing and scaling the continuse attributes, we should take care of the categorical attributes. Let's first print the number of distict values of the categirical attribute `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "renamedHousing.select(\"ocean_proximity\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String indexer\n",
    "Most Machine Learning algorithms prefer to work with numbers. So let's convert the categorical attribute `ocean_proximity` to numbers. To do so, we can use the `StringIndexer` that encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|catInd|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|   3.0|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|   3.0|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|   3.0|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|   3.0|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|   3.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexer = strIdx_6890ef73a1e9\n",
       "idxHousing = [longitude: double, latitude: double ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 12 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(colCat).setOutputCol(\"catInd\")\n",
    "val idxHousing = indexer.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "idxHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this numerical data in any Machine Learning algorithm. You can look at the mapping that this encoder has learned using the `labels` method: \"<1H OCEAN\" is mapped to 0, \"INLAND\" is mapped to 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(<1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.fit(renamedHousing).labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "Now, convert the label indices built in the last step into one-hot vectors. To do this, you can take advantage of the `OneHotEncoderEstimator` Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+------+-------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|catInd|    catEncode|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+------+-------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|   3.0|(4,[3],[1.0])|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|   3.0|(4,[3],[1.0])|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|   3.0|(4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|   3.0|(4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|   3.0|(4,[3],[1.0])|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder = oneHotEncoder_bb7f98f15112\n",
       "ohHousing = [longitude: double, latitude: double ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 13 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "\n",
    "\n",
    "val encoder = new OneHotEncoderEstimator().setInputCols(Array(\"catInd\")).setOutputCols(Array(\"catEncode\"))\n",
    "val ohHousing = encoder.fit(idxHousing).transform(idxHousing)\n",
    "\n",
    "ohHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Pipeline\n",
    "As you can see, there are many data transformation steps that need to be executed in the right order. For example, you called the `Imputer`, `VectorAssembler`, and `StandardScaler` from left to right. However, we can use the `Pipeline` class to define a sequence of Transformers/Estimators, and run them in order. A `Pipeline` is an `Estimator`, thus, after a Pipeline's `fit()` method runs, it produces a `PipelineModel`, which is a `Transformer`.\n",
    "\n",
    "Now, let's create a pipeline called `numPipeline` to call the numerical transformers you built above (`imputer`, `va`, and `scaler`) in the right order from left to right, as well as a pipeline called `catPipeline` to call the categorical transformers (`indexer` and `encoder`). Then, put these two pipelines `numPipeline` and `catPipeline` into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+------+-------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|        featuresFlat|              scalar|catInd|    catEncode|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+------+-------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|   3.0|(4,[3],[1.0])|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-122.22,37.86,21...|[-61.002278409814...|   3.0|(4,[3],[1.0])|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-122.24,37.85,52...|[-61.012260782324...|   3.0|(4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-122.25,37.85,52...|[-61.017251968579...|   3.0|(4,[3],[1.0])|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-122.25,37.85,52...|[-61.017251968579...|   3.0|(4,[3],[1.0])|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numPipeline = pipeline_9f6d5e7089c5\n",
       "catPipeline = pipeline_7e277e2ac6c6\n",
       "pipeline = pipeline_3e2fc1665622\n",
       "newHousing = [longitude: double, latitude: double ... 15 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[longitude: double, latitude: double ... 15 more fields]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
    "\n",
    "val numPipeline = new Pipeline().setStages(Array(imputer, va,scaler))\n",
    "val catPipeline = new Pipeline().setStages(Array(indexer, encoder))\n",
    "val pipeline = new Pipeline().setStages(Array(numPipeline, catPipeline))\n",
    "val newHousing = pipeline.fit(renamedHousing).transform(renamedHousing)\n",
    "\n",
    "newHousing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `VectorAssembler` to put all attributes of the final dataset `newHousing` into a big vector, and call the new column `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ljava.lang.String;@7f977d8d\n",
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[-61.007269596069...|452600.0|\n",
      "|[-61.002278409814...|358500.0|\n",
      "|[-61.012260782324...|352100.0|\n",
      "|[-61.017251968579...|341300.0|\n",
      "|[-61.017251968579...|342200.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cols = Array(scalar, catEncode)\n",
       "va2 = vecAssembler_ee688e20ead3\n",
       "dataset1 = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "//val cols = newHousing.columns.filter(_ != colCat)\n",
    "val cols=Array(\"scalar\",\"catEncode\")\n",
    "println(cols);\n",
    "val va2 = new VectorAssembler().setInputCols(cols).setOutputCol(\"features\")\n",
    "val dataset1 = va2.transform(newHousing).select(\"features\", \"label\")\n",
    "\n",
    "dataset1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Make a model\n",
    "Here we going to make four different regression models:\n",
    "* Linear regression model\n",
    "* Decission tree regression\n",
    "* Random forest regression\n",
    "* Gradient-booster forest regression\n",
    "\n",
    "But, before giving the data to train a Machine Learning model, let's first split the data into training dataset (`trainSet`) with 80% of the whole data, and test dataset (`testSet`) with 20% of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[-62.005506847089...| 73200.0|\n",
      "|[-61.995524474579...| 66900.0|\n",
      "|[-61.985542102068...| 74600.0|\n",
      "|[-61.980550915813...|107000.0|\n",
      "|[-61.980550915813...| 72200.0|\n",
      "|[-61.980550915813...| 70200.0|\n",
      "|[-61.980550915813...| 70500.0|\n",
      "|[-61.975559729558...| 60000.0|\n",
      "|[-61.975559729558...| 75500.0|\n",
      "|[-61.970568543303...| 85100.0|\n",
      "|[-61.970568543303...| 85600.0|\n",
      "|[-61.970568543303...| 96000.0|\n",
      "|[-61.965577357048...|100600.0|\n",
      "|[-61.960586170793...| 75100.0|\n",
      "|[-61.960586170793...| 99600.0|\n",
      "|[-61.960586170793...|143400.0|\n",
      "|[-61.960586170793...|122400.0|\n",
      "|[-61.955594984538...| 92800.0|\n",
      "|[-61.945612612028...| 96100.0|\n",
      "|[-61.945612612028...| 90200.0|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainSet1 = [features: vector, label: double]\n",
       "testSet1 = [features: vector, label: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val Array(trainSet1, testSet1) = dataset1.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "testSet1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Linear regression model\n",
    "Now, train a Linear Regression model using the `LinearRegression` class. Then, print the coefficients and intercept of the model, as well as the summary of the model over the training set by calling the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-55546.56043266049,-56452.04214475486,13692.654475868036,7241.058647006913,2937.7422557907666,-46634.74195692103,41227.15737469756,77970.63361154216,6103.1243180291995,15952.731501948629,750.5776374053479,-166119.78762122296,-200071.68439523797,-161342.38161209383,-170738.32659362792], Intercept: -2260083.9871313805\n",
      "RMSE: 67476.07036192763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr1 = linReg_44cc73930bcd\n",
       "lrModel1 = linReg_44cc73930bcd\n",
       "trainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@9b6efac\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.ml.regression.LinearRegressionTrainingSummary@9b6efac"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "// train the model\n",
    "val lr1 = new LinearRegression().setMaxIter(10)\n",
    ".setFeaturesCol(\"features\").setLabelCol(\"label\").setSolver(\"normal\")\n",
    "val lrModel1 = lr1.fit(trainSet1)\n",
    "val trainingSummary = lrModel1.summary\n",
    "\n",
    "println(s\"Coefficients: ${lrModel1.coefficients}, Intercept: ${lrModel1.intercept}\")\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `RegressionEvaluator` to measure the root-mean-square-erroe (RMSE) of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "|114636.98888270231| 73200.0|[-62.005506847089...|\n",
      "|107105.56739898026| 66900.0|[-61.995524474579...|\n",
      "| 93955.01353170304| 74600.0|[-61.985542102068...|\n",
      "|180167.20266358973|107000.0|[-61.980550915813...|\n",
      "|154537.76002162974| 72200.0|[-61.980550915813...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 69537.35484194856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_9b374f0f7486\n",
       "rmse = 69537.35484194856\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "69537.35484194856"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = lrModel1.transform(testSet1)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    ".setPredictionCol(\"prediction\")\n",
    ".setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Decision tree regression\n",
    "Repeat what you have done on Regression Model to build a Decision Tree model. Use the `DecisionTreeRegressor` to make a model and then measure its RMSE on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "|165532.87893175075| 73200.0|[-62.005506847089...|\n",
      "|137563.56144067796| 66900.0|[-61.995524474579...|\n",
      "|147028.97959183675| 74600.0|[-61.985542102068...|\n",
      "|165532.87893175075|107000.0|[-61.980550915813...|\n",
      "|165532.87893175075| 72200.0|[-61.980550915813...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 68421.60767593021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt = dtr_985acf79c571\n",
       "dtModel = DecisionTreeRegressionModel (uid=dtr_985acf79c571) of depth 5 with 63 nodes\n",
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_0cdd02bb529d\n",
       "rmse = 68421.60767593021\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "68421.60767593021"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val dt = new DecisionTreeRegressor().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "// train the model\n",
    "val dtModel = dt.fit(trainSet1)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = dtModel.transform(testSet1)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Random forest regression\n",
    "Let's try the test error on a Random Forest Model. Youcan use the `RandomForestRegressor` to make a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "| 172967.7321409177| 73200.0|[-62.005506847089...|\n",
      "|145430.12284887998| 66900.0|[-61.995524474579...|\n",
      "| 147053.0859961635| 74600.0|[-61.985542102068...|\n",
      "|172519.32045489136|107000.0|[-61.980550915813...|\n",
      "|147418.34493311186| 72200.0|[-61.980550915813...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 66104.413007993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf = rfr_305a748d4d0b\n",
       "rfModel = RandomForestRegressionModel (uid=rfr_305a748d4d0b) with 20 trees\n",
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_44514d253545\n",
       "rmse = 66104.413007993\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "66104.413007993"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val rf = new RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "// train the model\n",
    "val rfModel = rf.fit(trainSet1)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = rfModel.transform(testSet1)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    ".setPredictionCol(\"prediction\")\n",
    ".setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Gradient-boosted tree regression\n",
    "Fianlly, we want to build a Gradient-boosted Tree Regression model and test the RMSE of the test data. Use the `GBTRegressor` to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "|131338.82462455294| 73200.0|[-62.005506847089...|\n",
      "|105628.86788609039| 66900.0|[-61.995524474579...|\n",
      "| 71719.27815297342| 74600.0|[-61.985542102068...|\n",
      "|103366.86617259108|107000.0|[-61.980550915813...|\n",
      "| 98961.09540183515| 72200.0|[-61.980550915813...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 55542.62529156017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gb = gbtr_9cc222e6590c\n",
       "gbModel = GBTRegressionModel (uid=gbtr_9cc222e6590c) with 20 trees\n",
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_211fcf327b83\n",
       "rmse = 55542.62529156017\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "55542.62529156017"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.GBTRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val gb = new GBTRegressor().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "// train the model\n",
    "val gbModel = gb.fit(trainSet1)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = gbModel.transform(testSet1)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    ".setPredictionCol(\"prediction\")\n",
    ".setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Hyperparameter tuning\n",
    "An important task in Machie Learning is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LinearRegression, or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately. MLlib supports model selection tools, such as `CrossValidator`. These tools require the following items:\n",
    "* Estimator: algorithm or Pipeline to tune (`setEstimator`)\n",
    "* Set of ParamMaps: parameters to choose from, sometimes called a \"parameter grid\" to search over (`setEstimatorParamMaps`)\n",
    "* Evaluator: metric to measure how well a fitted Model does on held-out test data (`setEvaluator`)\n",
    "\n",
    "`CrossValidator` begins by splitting the dataset into a set of folds, which are used as separate training and test datasets. For example with `k=3` folds, `CrossValidator` will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular `ParamMap`, `CrossValidator` computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs. After identifying the best `ParamMap`, `CrossValidator` finally re-fits the Estimator using the best ParamMap and the entire dataset.\n",
    "\n",
    "Below, use the `CrossValidator` to select the best Random Forest model. To do so, you need to define a grid of parameters. Let's say we want to do the search among the different number of trees (1, 5, and 10), and different tree depth (5, 10, and 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "|114636.98888270231| 73200.0|[-62.005506847089...|\n",
      "|107105.56739898026| 66900.0|[-61.995524474579...|\n",
      "| 93955.01353170304| 74600.0|[-61.985542102068...|\n",
      "|180167.20266358973|107000.0|[-61.980550915813...|\n",
      "|154537.76002162974| 72200.0|[-61.980550915813...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 69537.35484194856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\trfr_305a748d4d0b-maxDepth: 5,\n",
       "\trfr_305a748d4d0b-numTrees: 1\n",
       "}, {\n",
       "\trfr_305a748d4d0b-maxDepth: 5,\n",
       "\trfr_305a748d4d0b-numTrees: 5\n",
       "}, {\n",
       "\trfr_305a748d4d0b-maxDepth: 5,\n",
       "\trfr_305a748d4d0b-numTrees: 10\n",
       "}, {\n",
       "\trfr_305a748d4d0b-maxDepth: 10,\n",
       "\trfr_305a748d4d0b-numTrees: 1\n",
       "}, {\n",
       "\trfr_305a748d4d0b-maxDepth: 10,\n",
       "\trfr_305a748d4d0b-numTrees: 5\n",
       "}, {\n",
       "\trfr_305a748d4d0b-maxDepth: 10,\n",
       "\trfr_305a748d4d0b-numTrees: 10\n",
       "}, {\n",
       "\trfr_305a748d4d0b-maxDepth: 15,\n",
       "\trfr_305a748d4d0b-numTrees: 1\n",
       "}, {\n",
       "\trfr_305a748d4d0b-maxDepth: 15,\n",
       "\trfr_305a748d4d0b-numTrees: 5\n",
       "}, {\n",
       "\trfr_305a748d4d0b-maxDepth: 15,\n",
       "\trfr_305a...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().addGrid(rf.numTrees, Array(1,5,10))\n",
    "    .addGrid(rf.maxDepth, Array(5,10,15))\n",
    "    .build()\n",
    "\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    "val cv = new CrossValidator().setEstimator(lr1)\n",
    "    .setEvaluator(new RegressionEvaluator())\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setNumFolds(3)\n",
    "val cvModel = cv.fit(trainSet1)\n",
    "\n",
    "val predictions = cvModel.transform(testSet1)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Custom transformer\n",
    "At the end of part two, we added extra columns to the `housing` dataset. Here, we are going to implement a Transformer to do the same task. The Transformer should take the name of two input columns `inputCol1` and `inputCol2`, as well as the name of ouput column `outputCol`. It, then, computes `inputCol1` divided by `inputCol2`, and adds its result as a new column to the dataset. The details of the implemeting a custom Tranfomer is explained [here](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types). Please read it before before starting to implement it.\n",
    "\n",
    "First, define the given parameters of the Transformer and implement a method to validate their schemas (`StructType`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait MyParams\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, DoubleType}\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "\n",
    "trait MyParams extends Params {\n",
    "    final val inputCol1 = new Param[String](this, \"inputCol1\",\"input colum1\")\n",
    "    final val inputCol2 = new Param[String](this, \"inputCol2\",\"input colum2\")\n",
    "    final val outputCol = new Param[String](this, \"outputCol\",\"output col\")\n",
    "    \n",
    "  protected def validateAndTransformSchema(schema: StructType): StructType = {\n",
    "       val idx = schema.fieldIndex($(inputCol1))\n",
    "      val idx2 = schema.fieldIndex($(inputCol2))\n",
    "    val field = schema.fields(idx)\n",
    "      val field2 = schema.fields(idx2)\n",
    "    if (field.dataType != DoubleType || field2.dataType != DoubleType) {\n",
    "      throw new Exception(s\"Input type ${field.dataType} did not match input type DoubleType\")\n",
    "    }\n",
    "    // Add the return field\n",
    "    schema.add(StructField($(outputCol), DoubleType, false))\n",
    "      \n",
    "   \n",
    "      \n",
    "  \n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extend the class `Transformer`, and implement its setter functions for the input and output columns, and call then `setInputCol1`, `setInputCol2`, and `setOutputCol`. Morever, you need to override the methods `copy`, `transformSchema`, and the `transform`. The details of what you need to cover in these methods is given [here](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MyTransformer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.util.Identifiable\n",
    "import org.apache.spark.ml.Transformer\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset}\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "\n",
    "class MyTransformer(override val uid: String) extends Transformer with MyParams {\n",
    "    def this() = this(Identifiable.randomUID(\"configurablewordcount\"))\n",
    "    \n",
    "    def setInputCol1(value: String): this.type=set(inputCol1,value)\n",
    "    \n",
    "    def setInputCol2(value: String): this.type=set(inputCol2,value)\n",
    "    \n",
    "    def setOutputCol(value: String): this.type=set(outputCol,value)\n",
    "\n",
    "    override def copy(extra: ParamMap) :MyTransformer=defaultCopy(extra)\n",
    "\n",
    "    override def transformSchema(schema: StructType) :StructType={\n",
    "        this.validateAndTransformSchema(schema: StructType)\n",
    "    }\n",
    "    \n",
    "    override def transform(dataset: Dataset[_]): DataFrame = {\n",
    "       val division = udf { (in1: Double,in2 :Double) => in1/in2 }\n",
    "   \n",
    "    dataset.withColumn($(outputCol), division(dataset.col($(inputCol1)),dataset.col($(inputCol2))) )    \n",
    "        \n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, an instance of `MyTransformer`, and set the input columns `total_rooms` and `households`, and the output column `rooms_per_household` and run it over the `housing` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|rooms_per_household|\n",
      "+-------------------+\n",
      "|  6.984126984126984|\n",
      "|  6.238137082601054|\n",
      "|  8.288135593220339|\n",
      "| 5.8173515981735155|\n",
      "|  6.281853281853282|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myTransformer = configurablewordcount_6eb1416e3139\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "myDataset: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "configurablewordcount_6eb1416e3139"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myTransformer = new MyTransformer().setInputCol1(\"total_rooms\").setInputCol2(\"households\").setOutputCol(\"rooms_per_household\")\n",
    "\n",
    "val myDataset = myTransformer.transform(housing).select(\"rooms_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Custom estimator (predictor)\n",
    "Now, it's time to implement your own linear regression with gradient descent algorithm as a brand new Estimator. The whole code of the Estimator is given to you, and you do not need to implement anything. It is just a sample that shows how to build a custom Estimator.\n",
    "\n",
    "The gradient descent update for linear regression is:\n",
    "$$\n",
    "w_{i+1} = w_{i} - \\alpha_{i} \\sum\\limits_{j=1}^n (w_i^\\top x_j - y_j)x_j\n",
    "$$\n",
    "\n",
    "where $i$ is the iteration number of the gradient descent algorithm, and $j$ identifies the observation. Here, $w$ represents an array of weights that is the same size as the array of features and provides a weight for each of the features when finally computing the label prediction in the form:\n",
    "\n",
    "$$\n",
    "prediction = w^\\top \\cdot\\ x\n",
    "$$\n",
    "\n",
    "where $w$ is the final array of weights computed by the gradient descent, $x$ is the array of features of the observation point and $prediction$ is the label we predict should be associated to this observation.\n",
    "\n",
    "The given `Helper` class implements the helper methods:\n",
    "* `dot`: implements the dot product of two vectors and the dot product of a vector and a scalar\n",
    "* `sum`: implements addition of two vectors\n",
    "* `fill`: creates a vector of predefined size and initialize it with the predefined value\n",
    "\n",
    "What you need to do is to implement the methods of the Linear Regresstion class `LR`, which are\n",
    "* `rmsd`: computes the Root Mean Square Error of a given RDD of tuples of (label, prediction) using the formula:\n",
    "$$\n",
    "rmse = \\sqrt{\\frac{\\sum\\limits_{i=1}^n (label - prediction)^2}{n}}\n",
    "$$\n",
    "* `gradientSummand`: computes the following formula:\n",
    "$$\n",
    "gs_{ij} = (w_i^\\top x_j - y_j)x_j\n",
    "$$\n",
    "* `gradient`: computes the following formula:\n",
    "$$\n",
    "gradient = \\sum\\limits_{j=1}^n gs_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Instance\n",
       "defined object Helper\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictorParams\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.util._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.linalg.Matrices\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.{PredictionModel, Predictor}\n",
    "\n",
    "case class Instance(label: Double, features: Vector)\n",
    "\n",
    "object Helper extends Serializable {\n",
    "  def dot(v1: Vector, v2: Vector): Double = {\n",
    "    val m = Matrices.dense(1, v1.size, v1.toArray)\n",
    "    m.multiply(v2).values(0)\n",
    "  }\n",
    "\n",
    "  def dot(v: Vector, s: Double): Vector = {\n",
    "    val baseArray = v.toArray.map(vi => vi * s)\n",
    "    Vectors.dense(baseArray)\n",
    "  }\n",
    "\n",
    "  def sumVectors(v1: Vector, v2: Vector): Vector = {\n",
    "    val baseArray = ((v1.toArray) zip (v2.toArray)).map { case (val1, val2) => val1 + val2 }\n",
    "    Vectors.dense(baseArray)\n",
    "  }\n",
    "\n",
    "  def fillVector(size: Int, fillVal: Double): Vector = Vectors.dense(Array.fill[Double](size)(fillVal));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class LR\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictorParams\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.util._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.linalg.Matrices\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.{PredictionModel, Predictor}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LR() extends Serializable {\n",
    "  def calcRMSE(labelsAndPreds: RDD[(Double, Double)]): Double = {\n",
    "    val regressionMetrics = new RegressionMetrics(labelsAndPreds)\n",
    "    regressionMetrics.rootMeanSquaredError\n",
    "  }\n",
    "  \n",
    "  def gradientSummand(weights: Vector, lp: Instance): Vector = {\n",
    "    val mult = (Helper.dot(weights, lp.features) - lp.label)\n",
    "    val seq = (0 to lp.features.size - 1).map(i => lp.features(i) * mult)\n",
    "    return Vectors.dense(seq.toArray)\n",
    "  }\n",
    "  \n",
    "  def linregGradientDescent(trainData: RDD[Instance], numIters: Int): (Vector, Array[Double]) = {\n",
    "    val n = trainData.count()\n",
    "    val d = trainData.take(1)(0).features.size\n",
    "    var w = Helper.fillVector(d, 0)\n",
    "    val alpha = 1.0\n",
    "    val errorTrain = Array.fill[Double](numIters)(0.0)\n",
    "\n",
    "    for (i <- 0 until numIters) {\n",
    "      val labelsAndPredsTrain = trainData.map(lp => (lp.label, Helper.dot(w, lp.features)))\n",
    "      errorTrain(i) = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "      val gradient = trainData.map(lp => gradientSummand(w, lp)).reduce((v1, v2) => Helper.sumVectors(v1, v2))\n",
    "      val alpha_i = alpha / (n * scala.math.sqrt(i + 1))\n",
    "      val wAux = Helper.dot(gradient, (-1) * alpha_i)\n",
    "      w = Helper.sumVectors(w, wAux)\n",
    "    }\n",
    "    (w, errorTrain)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MyLinearModel\n",
       "defined class MyLinearModelImpl\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictorParams\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.util._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.linalg.Matrices\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.{PredictionModel, Predictor}\n",
    "\n",
    "\n",
    "abstract class MyLinearModel[FeaturesType, Model <: MyLinearModel[FeaturesType, Model]]\n",
    "  extends PredictionModel[FeaturesType, Model] {\n",
    "}\n",
    "\n",
    "class MyLinearModelImpl(override val uid: String, val weights: Vector, val trainingError: Array[Double])\n",
    "    extends MyLinearModel[Vector, MyLinearModelImpl] {\n",
    "\n",
    "  override def copy(extra: ParamMap): MyLinearModelImpl = defaultCopy(extra)\n",
    "\n",
    "  def predict(features: Vector): Double = {\n",
    "    println(\"Predicting\")\n",
    "    val prediction = Helper.dot(weights, features)\n",
    "    prediction\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MyLinearRegression\n",
       "defined class MyLinearRegressionImpl\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictorParams\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.util._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.linalg.Matrices\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.{PredictionModel, Predictor}\n",
    "\n",
    "abstract class MyLinearRegression[\n",
    "    FeaturesType,\n",
    "    Learner <: MyLinearRegression[FeaturesType, Learner, Model],\n",
    "    Model <: MyLinearModel[FeaturesType, Model]]\n",
    "  extends Predictor[FeaturesType, Learner, Model] {\n",
    "}\n",
    "\n",
    "class MyLinearRegressionImpl(override val uid: String)\n",
    "    extends MyLinearRegression[Vector, MyLinearRegressionImpl, MyLinearModelImpl] {\n",
    "  def this() = this(Identifiable.randomUID(\"linReg\"))\n",
    "\n",
    "  override def copy(extra: ParamMap): MyLinearRegressionImpl = defaultCopy(extra)\n",
    "  \n",
    "  def train(dataset: Dataset[_]): MyLinearModelImpl = {\n",
    "    println(\"Training\")\n",
    "\n",
    "    val numIters = 10\n",
    "\n",
    "    val instances: RDD[Instance] = dataset.select(\n",
    "      col($(labelCol)), col($(featuresCol))).rdd.map {\n",
    "        case Row(label: Double, features: Vector) =>\n",
    "          Instance(label, features)\n",
    "      }\n",
    "\n",
    "    val (weights, trainingError) = new LR().linregGradientDescent(instances, numIters)\n",
    "\n",
    "    new MyLinearModelImpl(uid, weights, trainingError)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "+--------------------+--------+--------------------+\n",
      "|          prediction|   label|            features|\n",
      "+--------------------+--------+--------------------+\n",
      "|-8.61613519035840...| 73200.0|[-62.005506847089...|\n",
      "|-8.62150999011452...| 66900.0|[-61.995524474579...|\n",
      "|-8.61747570123055...| 74600.0|[-61.985542102068...|\n",
      "|-8.59161184468638...|107000.0|[-61.980550915813...|\n",
      "|-8.59471883851774E37| 72200.0|[-61.980550915813...|\n",
      "+--------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 8.235481859085347E37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr = linReg_6dd5a7674c56\n",
       "model = linReg_6dd5a7674c56\n",
       "predictions = [features: vector, label: double ... 1 more field]\n",
       "evaluator = regEval_8c0f7a82a52a\n",
       "rmse = 8.235481859085347E37\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8.235481859085347E37"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val lr = new MyLinearRegressionImpl().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "val model = lr.fit(trainSet1)\n",
    "val predictions = model.transform(testSet1)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. An End-to-End Classification Test\n",
    "As the last step, you are given a dataset called `data/ccdefault.csv`. The dataset represents default of credit card clients.It has 30,000 cases and 24 different attributes. More details about the dataset is available at `data/ccdefault.txt`. In this task you should make three models, compare their results and conclude the ideal solution. Here are the suggested steps:\n",
    "1. Load the data.\n",
    "2. Carry out some exploratory analyses (e.g., how various features and the target variable are distributed).\n",
    "3. Train a model to predict thetarget variable (risk of `default`).\n",
    "  - Employ three different models (logistic regression, decision tree, and random forest).\n",
    "  - Compare the models' performances (e.g., AUC).\n",
    "  - Defend your choice of best model (e.g., what are the strength and weaknesses of each of these models?).\n",
    "4. What more would you do with this data? Anything to help you devise a better solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [ID: int, LIMIT_BAL: int ... 23 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 23 more fields]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\")load(\"data/ccdefault.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|DEFAULT|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|      1|\n",
      "|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|      1|\n",
      "|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|      0|\n",
      "|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|      0|\n",
      "|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|      0|\n",
      "|  6|    50000|  1|        1|       2| 37|    0|    0|    0|    0|    0|    0|    64400|    57069|    57608|    19394|    19619|    20024|    2500|    1815|     657|    1000|    1000|     800|      0|\n",
      "|  7|   500000|  1|        1|       2| 29|    0|    0|    0|    0|    0|    0|   367965|   412023|   445007|   542653|   483003|   473944|   55000|   40000|   38000|   20239|   13750|   13770|      0|\n",
      "|  8|   100000|  2|        2|       2| 23|    0|   -1|   -1|    0|    0|   -1|    11876|      380|      601|      221|     -159|      567|     380|     601|       0|     581|    1687|    1542|      0|\n",
      "|  9|   140000|  2|        3|       1| 28|    0|    0|    2|    0|    0|    0|    11285|    14096|    12108|    12211|    11793|     3719|    3329|       0|     432|    1000|    1000|    1000|      0|\n",
      "| 10|    20000|  1|        3|       2| 35|   -2|   -2|   -2|   -2|   -1|   -1|        0|        0|        0|        0|    13007|    13912|       0|       0|       0|   13007|    1122|       0|      0|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(ID, LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, DEFAULT)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+-----------------+\n",
      "|               ID|         LIMIT_BAL|               SEX|              AGE|\n",
      "+-----------------+------------------+------------------+-----------------+\n",
      "|            30000|             30000|             30000|            30000|\n",
      "|          15000.5|167484.32266666667|1.6037333333333332|          35.4855|\n",
      "|8660.398374208891|129747.66156720246|0.4891291960902602|9.217904068090155|\n",
      "|                1|             10000|                 1|               21|\n",
      "|            30000|           1000000|                 2|               79|\n",
      "+-----------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().select(\"ID\",\"LIMIT_BAL\",\"SEX\",\"AGE\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "renamedData = [ID: int, LIMIT_BAL: int ... 23 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 23 more fields]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val renamedData = data.withColumnRenamed(\"DEFAULT\",\"label\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamedData.select(\"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colLabel = label\n",
       "colNum = Array(ID, LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(ID, LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colLabel = \"label\"\n",
    "val colNum = renamedData.columns.filter(_ != colLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamedData.select(\"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\n",
      "0\n",
      "LIMIT_BAL\n",
      "0\n",
      "SEX\n",
      "0\n",
      "EDUCATION\n",
      "0\n",
      "MARRIAGE\n",
      "0\n",
      "AGE\n",
      "0\n",
      "PAY_0\n",
      "0\n",
      "PAY_2\n",
      "0\n",
      "PAY_3\n",
      "0\n",
      "PAY_4\n",
      "0\n",
      "PAY_5\n",
      "0\n",
      "PAY_6\n",
      "0\n",
      "BILL_AMT1\n",
      "0\n",
      "BILL_AMT2\n",
      "0\n",
      "BILL_AMT3\n",
      "0\n",
      "BILL_AMT4\n",
      "0\n",
      "BILL_AMT5\n",
      "0\n",
      "BILL_AMT6\n",
      "0\n",
      "PAY_AMT1\n",
      "0\n",
      "PAY_AMT2\n",
      "0\n",
      "PAY_AMT3\n",
      "0\n",
      "PAY_AMT4\n",
      "0\n",
      "PAY_AMT5\n",
      "0\n",
      "PAY_AMT6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for (c <- colNum) {\n",
    "    println(c)\n",
    "   val a =renamedData.where(renamedData.col(c).isNull).count()\n",
    "\n",
    "   println(a) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|label|        featuresFlat|              scalar|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|    1|[1.0,20000.0,2.0,...|[1.15468129385139...|\n",
      "|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|    1|[2.0,120000.0,2.0...|[2.30936258770278...|\n",
      "|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|    0|[3.0,90000.0,2.0,...|[3.46404388155417...|\n",
      "|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|    0|[4.0,50000.0,2.0,...|[4.61872517540556...|\n",
      "|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|    0|[5.0,50000.0,1.0,...|[5.77340646925695...|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va = vecAssembler_2da0039b0c94\n",
       "featuredData = [ID: int, LIMIT_BAL: int ... 24 more fields]\n",
       "scaler = stdScal_a56ac9351a15\n",
       "scaledData = [ID: int, LIMIT_BAL: int ... 25 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ID: int, LIMIT_BAL: int ... 25 more fields]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(colNum).setOutputCol(\"featuresFlat\")\n",
    "val featuredData = va.transform(renamedData)\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"featuresFlat\").setOutputCol(\"scalar\")\n",
    "val scaledData = scaler.fit(featuredData).transform(featuredData)\n",
    "\n",
    "scaledData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(ID, LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, label, featuresFlat, scalar)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.select(\"label\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ljava.lang.String;@6604b1cf"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cols = Array(scalar)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(scalar)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cols=Array(\"scalar\")\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "va2 = vecAssembler_7d46b23bfa94\n",
       "dataset = [features: vector, label: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: int]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val newData = pipeline.fit(scaledData).transform(scaledData)\n",
    "\n",
    "val va2 = new VectorAssembler().setInputCols(cols).setOutputCol(\"features\")\n",
    "val dataset = va2.transform(scaledData).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(24,[0,1,2,3,4,5,...|    0|\n",
      "|(24,[0,1,2,3,4,5,...|    0|\n",
      "|(24,[0,1,2,3,4,5,...|    1|\n",
      "|(24,[0,1,2,3,4,5,...|    1|\n",
      "|(24,[0,1,2,3,4,5,...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainSet = [features: vector, label: int]\n",
       "testSet = [features: vector, label: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: int]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainSet, testSet) = dataset.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "trainSet.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics \n",
    "import org.apache.spark.ml.evaluation.Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@5ac37fdd"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr = logreg_89800cc482fc\n",
       "lrModel = LogisticRegressionModel: uid = logreg_89800cc482fc, numClasses = 2, numFeatures = 24\n",
       "trainingSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@5ac37fdd\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@5ac37fdd"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    ".setFeaturesCol(\"features\").setLabelCol(\"label\")\n",
    "val lrModel = lr.fit(trainSet)\n",
    "\n",
    "val trainingSummary = lrModel.summary\n",
    "print(trainingSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is = 0.8100913621262459"
     ]
    }
   ],
   "source": [
    "print(s\"The accuracy of the classifier is = ${trainingSummary.accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(24,[0,1,2,3,4,5,...|    1|[0.76063703527588...|[0.68149202506091...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|    0|[1.26501454083979...|[0.77988812512979...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|    0|[1.07594848893447...|[0.74572650707394...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|    0|[1.05867495812928...|[0.74243724666218...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|    0|[0.96818510686525...|[0.72475760333197...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions = [features: vector, label: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: int ... 3 more fields]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = lrModel.transform(testSet)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp = [label: int, prediction: double]\n",
       "counttotal = 5920\n",
       "correct = 4797\n",
       "wrong = 1123\n",
       "truep = 4479\n",
       "falseN = 1013\n",
       "falseP = 110\n",
       "ratioWrong = 0.18969594594594594\n",
       "ratioCorrect = 0.810304054054054\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.810304054054054"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count()\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7166510122284702"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator1 = binEval_5e8481264953\n",
       "accuracy = 0.7166510122284702\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7166510122284702"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "val evaluator1 = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "// Evaluates predictions and returns a scalar metric areaUnderROC(larger is better).**\n",
    "// val evaluator = new BinaryClassificationEvaluator()\n",
    "val accuracy = evaluator1.evaluate(predictions)\n",
    "print(accuracy)\n",
    "// val evaluator = new RegressionEvaluator().setMetricName(\"areaUnderROC\")\n",
    "// .setPredictionCol(\"prediction\")\n",
    "// .setLabelCol(\"label\")\n",
    "// val rmse = evaluator.evaluate(predictions)\n",
    "// println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"label\", \"features\").where(\"prediction == 0\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5272966369818625\n",
      "0.4836730483045375\n",
      "0.47913413708286284\n",
      "0.47830068188874914\n",
      "0.4739642512745035\n",
      "0.47019670851094714\n",
      "0.465899269083548\n",
      "0.46560632204846114\n",
      "0.46488666912752896\n",
      "0.4647789000291914\n",
      "0.4646365390655921\n",
      "0.46450610082117777\n",
      "0.4643777851040914\n",
      "0.4643220713897213\n",
      "0.46429359956426536\n",
      "0.46427850151385563\n",
      "0.4642481615564603\n",
      "0.4642255750686248\n",
      "0.46419212638715707\n",
      "0.46415595829210426\n",
      "0.4640873000363526\n",
      "0.46405523492633505\n",
      "0.46401047176093213\n",
      "0.4639910215941\n",
      "0.46397176090796344\n",
      "0.4639687994148078\n",
      "0.4639561119834347\n",
      "0.46394812348773506\n",
      "0.4639413277601859\n",
      "0.4639373931472524\n",
      "0.4639319873034502\n",
      "0.4639256775736155\n",
      "0.4639216214895904\n",
      "0.4639188201544619\n",
      "0.463915801762874\n",
      "0.46391001624256706\n",
      "0.4639094237630225\n",
      "0.46390708496060157\n",
      "0.4639056983116319\n",
      "0.4639035788848467\n",
      "0.46390196766992714\n",
      "0.4639014403003657\n",
      "0.4639006537611187\n",
      "0.4639002882383104\n",
      "0.4639001769540668\n",
      "0.46390015173910526\n",
      "0.46390002830437355\n",
      "0.46390001534417863\n",
      "0.46389997656619086\n",
      "0.4638999262553854\n",
      "0.4638998902090737\n",
      "0.4638998327208819\n",
      "0.4638997230743682\n",
      "0.46389962790998374\n",
      "0.4638995409858975\n",
      "0.4638995161843547\n",
      "0.4638994945426393\n",
      "0.46389948536634984\n",
      "0.46389947697418243\n",
      "0.4638994688585948\n",
      "0.4638994637807744\n",
      "0.46389946132617343\n",
      "0.4638994594965155\n",
      "0.4638994570037381\n",
      "0.4638994561228328\n",
      "0.4638994556698238\n",
      "0.4638994547394173\n",
      "0.4638994545792143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "objectiveHistory = Array(0.5272966369818625, 0.4836730483045375, 0.47913413708286284, 0.47830068188874914, 0.4739642512745035, 0.47019670851094714, 0.465899269083548, 0.46560632204846114, 0.46488666912752896, 0.4647789000291914, 0.4646365390655921, 0.46450610082117777, 0.4643777851040914, 0.4643220713897213, 0.46429359956426536, 0.46427850151385563, 0.4642481615564603, 0.4642255750686248, 0.46419212638715707, 0.46415595829210426, 0.4640873000363526, 0.46405523492633505, 0.46401047176093213, 0.4639910215941, 0.46397176090796344, 0.4639687994148078, 0.4639561119834347, 0.46394812348773506, 0.4639413277601859, 0.4639373931472524, 0.4639319873034502, 0.4639256775736155, 0.4639216214895904, 0.4639188201544619, 0.463915801762874, 0.46391001624256706, 0.4639094237630225, 0.46390...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(0.5272966369818625, 0.4836730483045375, 0.47913413708286284, 0.47830068188874914, 0.4739642512745035, 0.47019670851094714, 0.465899269083548, 0.46560632204846114, 0.46488666912752896, 0.4647789000291914, 0.4646365390655921, 0.46450610082117777, 0.4643777851040914, 0.4643220713897213, 0.46429359956426536, 0.46427850151385563, 0.4642481615564603, 0.4642255750686248, 0.46419212638715707, 0.46415595829210426, 0.4640873000363526, 0.46405523492633505, 0.46401047176093213, 0.4639910215941, 0.46397176090796344, 0.4639687994148078, 0.4639561119834347, 0.46394812348773506, 0.4639413277601859, 0.4639373931472524, 0.4639319873034502, 0.4639256775736155, 0.4639216214895904, 0.4639188201544619, 0.463915801762874, 0.46391001624256706, 0.4639094237630225, 0.46390..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val objectiveHistory = trainingSummary.objectiveHistory\n",
    "objectiveHistory.foreach(loss => println(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                 FPR|                TPR|\n",
      "+--------------------+-------------------+\n",
      "|                 0.0|                0.0|\n",
      "|0.003462050599201...|0.03298774740810556|\n",
      "|0.007456724367509...| 0.0640904806786051|\n",
      "| 0.01081225033288948|0.09745523091423186|\n",
      "|0.014007989347536618|0.13138548539114042|\n",
      "|0.017949400798934752|0.16267672007540057|\n",
      "|0.021784287616511317| 0.1943449575871819|\n",
      "|0.025459387483355526|0.22657869934024505|\n",
      "| 0.03062583222370173| 0.2535344015080113|\n",
      "|0.035525965379494005| 0.2814326107445806|\n",
      "| 0.04047936085219707| 0.3091423185673893|\n",
      "| 0.04585885486018642| 0.3353440150801131|\n",
      "| 0.05171770972037284| 0.3598491988689915|\n",
      "| 0.05821571238348868| 0.3820923656927427|\n",
      "| 0.06545938748335553|0.40169651272384543|\n",
      "| 0.07344873501997337| 0.4186616399622997|\n",
      "| 0.08154460719041279| 0.4352497643732328|\n",
      "| 0.09049267643142477| 0.4488218661639962|\n",
      "| 0.09880159786950732| 0.4646559849198869|\n",
      "| 0.10737683089214381|0.47954759660697455|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "0.7254747989171835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "binarySummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@5ac37fdd\n",
       "roc = [FPR: double, TPR: double]\n",
       "fMeasure = [threshold: double, F-Measure: double]\n",
       "maxFMeasure = 0.5187046882551958\n",
       "bestThreshold = 0.27584584603559487\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = logreg_89800cc482fc, numClasses = 2, numFeatures = 24"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Obtain the metrics useful to judge performance on test data.\n",
    "// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n",
    "// binary classification problem.\n",
    "val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
    "\n",
    "// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "val roc = binarySummary.roc\n",
    "roc.show()\n",
    "println(binarySummary.areaUnderROC)\n",
    "\n",
    "// Set the model threshold to maximize F-Measure\n",
    "val fMeasure = binarySummary.fMeasureByThreshold\n",
    "val maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n",
    "val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure)\n",
    "  .select(\"threshold\").head().getDouble(0)\n",
    "lrModel.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt = dtc_39a396c02f9a\n",
       "dtModel = DecisionTreeClassificationModel (uid=dtc_39a396c02f9a) of depth 5 with 41 nodes\n",
       "predictions = [features: vector, label: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: int ... 3 more fields]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "\n",
    "val dt = new DecisionTreeClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "// train the model\n",
    "val dtModel = dt.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = dtModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "// val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    "// .setPredictionCol(\"prediction\")\n",
    "// .setLabelCol(\"label\")\n",
    "// val rmse = evaluator.evaluate(predictions)\n",
    "// println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3211521229923122"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator = binEval_97de3e5c7c09\n",
       "accuracy = 0.3211521229923122\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3211521229923122"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp = [label: int, prediction: double]\n",
       "counttotal = 5920\n",
       "correct = 4840\n",
       "wrong = 1080\n",
       "truep = 4382\n",
       "falseN = 873\n",
       "falseP = 207\n",
       "ratioWrong = 0.18243243243243243\n",
       "ratioCorrect = 0.8175675675675675\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8175675675675675"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count()\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf = rfc_7d674a38cd22\n",
       "rfModel = RandomForestClassificationModel (uid=rfc_7d674a38cd22) with 10 trees\n",
       "predictions = [features: vector, label: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: vector, label: int ... 3 more fields]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)\n",
    "\n",
    "// train the model\n",
    "val rfModel = rf.fit(trainSet)\n",
    "\n",
    "//make predictions on the test data\n",
    "val predictions = rfModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(15)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "// val evaluator = new RegressionEvaluator().setMetricName(\"rmse\")\n",
    "// .setPredictionCol(\"prediction\")\n",
    "// .setLabelCol(\"label\")\n",
    "// val rmse = evaluator.evaluate(predictions)\n",
    "// println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    0|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|(24,[0,1,2,3,4,5,...|\n",
      "|       1.0|    1|[0.00265576697585...|\n",
      "|       1.0|    1|[0.00727449215126...|\n",
      "|       1.0|    0|[0.00808276905695...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"label\", \"features\").where(\"prediction == 1\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:140: error: not enough arguments for method apply: (index: Int)Char in class StringOps.\n",
       "Unspecified value parameter index.\n",
       "       evaluator.getMetricName()\n",
       "                              ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp = [label: int, prediction: double]\n",
       "counttotal = 5920\n",
       "correct = 4828\n",
       "wrong = 1092\n",
       "truep = 4393\n",
       "falseN = 896\n",
       "falseP = 196\n",
       "ratioWrong = 0.18445945945945946\n",
       "ratioCorrect = 0.8155405405405406\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8155405405405406"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count()\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
